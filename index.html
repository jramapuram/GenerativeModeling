<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Generative Modeling</title>

    <link rel="stylesheet" href="css/reveal.css">
    <!-- <link rel="stylesheet" href="css/theme/black.css"> -->
    <link rel="stylesheet" href="css/theme/blood.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

        <section>
          <h2>Generative Modeling</h2>
          <h4>Spring School 2017</h4>
          <br><br>
          <p>
            <img height="200" src="imgs/cui.gif">
            <br>
            <small>by <a href="https://jramapuram.github.io/">Jason Ramapuram</a> </small>
          </p>
        </section>

        <section>
          <aside class="notes">
            <ul>
              <li>A generative model can be transformed into a discriminative model via bayes rule</li>
            </ul>
          </aside>

          <h3>What is a Generative Model?</h3>
          <p>
            Given $X = \{x_i\}_{i=0}^M \in \mathcal{R}^{N \times M}$ and $Y =
            \{y_i\}_{i=0}^M \in \mathcal{R}^{N\times1}$
            <ol>
              <li>a generative model learns learns a joint distribution $P(X, Y)$. </li>
              <!-- <ul>
                   <li>Sometimes a latent variable is utilized: $Z = {z_i}_{i=0}^M \in \mathcal{R}^{N \times K}$</li>
                   </ul> -->
                   <li>a discriminative model on the other hand learns a conditional distribution $P(Y|X)$. </li>
            </ol>
          </p>
        </section>

        <section>
          <aside class="notes">
            <ul>
              <li></li>
            </ul>
          </aside>

          <h3>Why Learn Generative Modeling?</h3>
            <ul>
              <li>Generative time series models can be utilized to predict the future.</li>
              <li>Has the ability to fill in missing data: semi-supervised
                learning for example where some labels are missing or image
                inpainting</li>
              <li>GAN's for example allow the learning of multi-modal
                distributions; minimizing the mean squared error in comparison
                cannot learn to produce multiple correct answers
                (Goodfellow NIPS 2016)</li>
              <li>Unsupervised learning, denoising, super-resolution, compression, etc</li>
            </ul>
        </section>

        <section>
          <aside class="notes">
            <ul>
              <li>leaving aside computational issues and matters such as
                handling missing data, the prevailing consensus seems to be that
                discriminative classifiers are almost always to be preferred to
                generative ones.</li>
              <li>"it is known that sample complexity in the discriminative setting is linear in the VC dimension"</li>
            </ul>
          </aside>

          <h3>When not to use a Generative Model?</h3>
          <p>
            "one should solve the [classification] problem directly and never
            solve a more general problem as an intermediate step (such as
            modeling P(X|Y)]." (Vapnik 1998)
          </p>
        </section>


        <section>
          <aside class="notes">
            <ul>
              <li></li>
            </ul>
          </aside>

          <h3>Unsupervised Generative Modeling</h3>
          <br>
          <p>
            Given $X = \{x_i\}_{i=0}^M \in \mathcal{R}^{N \times M}$ distributed as $P(X)$ (unknown), an unsupervised generative model learns a approximation $P_{\theta}(X)$
          </p>
        </section>

        <section>
          <aside class="notes">
            <ul>
              <li>Left of the tree explicitly models density $p_{model}(x;
              \theta)$ and you can thus explicitly minimize the likelihood</li>
              <li>Typically the indirect means of interacting with the probability
              distribution is the ability to draw samples from it. Some of these
              implicit models that o er the ability to sample from the
              distribution do so using a MarkovChain; the model de nes a way to
              stochastically transform an existing sample in orderto obtain
              another sample from the same distribution. Others are able to
              generate asample in a single step, starting without any input.
              </li>
              <li>GSN : generative stochastic networks basically try to learn
              the tranisition operator on a markov chain whose stationary
              distribution estimates the data distribution</li>
            </ul>
          </aside>

          <h3>Taxonomy of Unsupervised Generative Models</h3>
          <img src="imgs/landscape.png">
          <p><small>(Goodfellow NIPS 2016)</small></p>
        </section>

        <section>
          <aside class="notes">
            <ul>
              <li>This is used in wavenet (2mins for 1s of audio)</li>
              <li>Similar to RNNs</li>
              <li>Slow because samples need to be generated one at a time</li>
              <li>Sampling complexity is O(n)</li>
            </ul>
          </aside>

          <h3>Tractable + Explicit Density: FVBN</h3>
          <p>$$p_{model}(x) = \prod_{i=1}^{N} p_{model} (x_i | x_1, x_2, ..., x_{i-1})$$</p>
          <ul>
            <li>The FVBN (Fully Visible Belief Network) (Frey et al.,1996; Frey, 1998) utilize the chain rule to decompose a probability distribution over an n-dimensional vector x into a product of one-dimensional distributions (Goodfellow NIPS 2016)</li>
          </ul>
        </section>

        <section>
          <aside class="notes">
            <ul>
              <li> Models with nonlineargfunctions date back at least to Deco and Brauer(1995).</li>
              <li>The  latest  member  of  this  family  is  real  NVP(Dinhet  al., 2016).</li>
              <li>Specifically, designing models with tractable learning,
              sampling, inference and evaluation is crucial in solving this
              task. We extend the space of such models using real-valued
              non-volume preserving (real NVP) transformations, a set of
              powerful invertible and learnable transformations, resulting in an
              unsupervised learning algorithm with exact log-likelihood
              computation, exact sampling, exact inference of latent variables,
              and an interpretable latent space.</li>
              <li>The main drawback to nonlinear ICA models is that they impose
              restrictions on the choice of the functiong. In particular, the
              invertibility requirement means that the latent variables z must
              have the same dimensionality as x. </li
            </ul>
          </aside>

          <h3>Tractable + Explicit Density: Non-Linear ICA</h3>
            <p>$$p_x(x) = p_z (g^{-1}(x) \bigg{|} det \left( \frac{\delta g^{-1}(x)}{\delta x} \right) \bigg{|} $$</p>
          <ul>
            <li>assumes vector of latent variables $z$</li>
            <li>assumes existence of continuous, differentiable, invertible function $g(z)$</li>
            <li>requires tractable density $p_z$, tractable determinant of Jacobian of $g^{-1}$</li>
            <li>$g(z)$  yields a sample from the model in $x$ space</li>
          </ul>
          <p>TLDR: a simple distribution over $z$ combined with a transformation
          $g$ that warps space in complicated ways can yield a complicated
          distribution over $x$ (Dinh et al., 2016, Deco and Brauer 1995)</p>
        </section>

        <section>
          <aside class="notes">
            <ul>
              <li>Boltzman machines utilize MCMC</li>
              <li>q is the transition operator; no explicit way to test when it has converged</li>
              <li>GANs / VAEs for example provide a single step sampling as opposed to multi-stage from MCMC</li>
            </ul>
          </aside>

          <h3>Explicit Models that require Approximations</h3>
          <p>Provide density explicit functions, but use one that is intractable.</p>
          <ul>
            <li>Variational approximations such as the VAE; mean field inference</li>
            <ul>
              <li>Minimize the variational lower bound (AKA ELBO): $\mathcal{L}(x;\theta) \leq log p_{model}(x;\theta)$</li>
            <li>MCMC approximations: $x' \sim q(x' | x)$</li>
          </ul>
        </section>


        <section>
          <aside class="notes">
            <ul>
              <li></li>
            </ul>
          </aside>

          <h3>Classical VI Recipe</h3>
          <img src="imgs/old_recipe.png">
          <p><small>Blei et. al NIPS 2016</small></p>
        </section>

        <section>
          <aside class="notes">
            <ul>
              <li></li>
            </ul>
          </aside>

          <h3>Exercise: Bayesian Logistic Regression</h3>
          <ul>
            <li>For simplicity let's work with one data point: $(y, x)$</li>
            <li>Bayesian version puts a prior on the regression coefficient z</li>
            <ul>
              <li>Let's assume z is distributed as a unit gaussian: $p(z) \sim \mathcal{N}(0, 1)$</li>
            </ul>
            <li>Since we are doing binary classification: $p(y | x, z) \sim Bern(\sigma(z x))$</li>
            <ul>
              <li>Recall: $Bern(\phi ; \mu) = \mu^{\phi} (1 - \mu)^{1 - \phi}$</li>
            </ul>
            <li>Solve the ELBO: </li>
p            $\begin{align}
            \mathcal{L}(\mu, \sigma^2) &= \mathbb{E}_{q \sim \mathcal{N}(\mu, \sigma^2)} [log\ p(z) \\
            &- log\ q(z) + log\ p(y|x,z)]
            \end{align}$
          </ul>
        </section>

        <section>
          <aside class="notes">
            <ul>
              <li></li>
            </ul>
          </aside>

          <h3>New VI Recipe</h3>
          <img src="imgs/new_recipe.png">
          <p><small>Blei et. al NIPS 2016</small></p>
        </section>

        <section>
          <h3>References</h3>
          <ul>
            <li>Frey, B. J. (1998).Graphical  models  for  machine  learning  and  digital  communication.  MIT Press.</li>
            <li>Frey, B. J., Hinton, G. E., and Dayan, P. (1996). Does the wake-sleep algorithm learn good density estimators?  In D. Touretzky, M. Mozer, and M. Hasselmo,editors,Advances  in  Neural  Information  Processing  Systems  8  (NIPS'95),pages 661{670. MIT Press, Cambridge, MA.</li>
            <li>Dinh,  L.,  Sohl-Dickstein,  J.,  and Bengio,  S. (2016).  Density estimation usingreal nvp.arXiv preprint arXiv:1605.08803.</li>
            <li>Deco, G. and Brauer, W. (1995). Higher order statistical decorrelation without information loss. NIPS.</li>
          </ul>
        </section>






      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>

    <script>
      // More info https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        history: true,
        progress: true,
        slideNumber: true,

        math: {
          mathjax: 'https://cdn.mathjax.org/mathjax/latest/MathJax.js',
          config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
        },

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/math/math.js', async: true },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });
    </script>
  </body>
</html>
